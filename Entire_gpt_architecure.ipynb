{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"170ZyijbuJPXuc5-dXLTpaXhuM1MYFeSL","authorship_tag":"ABX9TyMOuIFsIOIiljf32ovYXr/3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"uF0WDsyalKSR","executionInfo":{"status":"ok","timestamp":1757060899206,"user_tz":-330,"elapsed":3849,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"z3tKOG7OY4ou","executionInfo":{"status":"ok","timestamp":1757060899224,"user_tz":-330,"elapsed":15,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"outputs":[],"source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,    # Vocabulary size\n","    \"context_length\": 1024, # Context length\n","    \"emb_dim\": 768,         # Embedding dimension\n","    \"n_heads\": 12,          # Number of attention heads\n","    \"n_layers\": 12,         # Number of layers\n","    \"drop_rate\": 0.1,       # Dropout rate\n","    \"qkv_bias\": False       # Query-Key-Value bias\n","}"]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out must be divisible by num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"Amg2ns33UAuc","executionInfo":{"status":"ok","timestamp":1757060899224,"user_tz":-330,"elapsed":13,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class GELU(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","  def forward(self,x):\n","    return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"],"metadata":{"id":"iDgHatDzf7YF","executionInfo":{"status":"ok","timestamp":1757060899225,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self,cfg):\n","    super().__init__()\n","    self.layers= nn.Sequential(\n","        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n","        GELU(),\n","        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n","    )\n","  def forward(self,x):\n","    return self.layers(x)"],"metadata":{"id":"GMfh8iiwe4nW","executionInfo":{"status":"ok","timestamp":1757060899225,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class LayerNorm(nn.Module):\n","  def __init__(self,emb_dim):\n","    super().__init__()\n","    self.eps=1e-5\n","    self.scale= nn.Parameter(torch.ones(emb_dim))\n","    self.shift= nn.Parameter(torch.zeros(emb_dim))\n","\n","  def forward(self,x):\n","    mean = x.mean(dim=-1,keepdim=True)\n","    var= x.var(dim=-1,keepdim=True,unbiased =False)\n","    norm_x= (x - mean)/ torch.sqrt(var+self.eps)\n","    return self.scale*norm_x + self.shift"],"metadata":{"id":"__8wIX5PgR0v","executionInfo":{"status":"ok","timestamp":1757060899238,"user_tz":-330,"elapsed":16,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","  def __init__(self,cfg):\n","    super().__init__()\n","    self.att = MultiHeadAttention(\n","        d_in=cfg['emb_dim'],\n","        d_out=cfg['emb_dim'],\n","        context_length=cfg['context_length'],\n","        num_heads=cfg['n_heads'],\n","        dropout=cfg['drop_rate'],\n","        qkv_bias=cfg['qkv_bias']\n","    )\n","    self.ff= FeedForward(cfg)\n","    self.norm1=LayerNorm(cfg['emb_dim'])\n","    self.norm2=LayerNorm(cfg['emb_dim'])\n","    self.drop_shortcut= nn.Dropout(cfg['drop_rate'])\n","\n","  def forward(self,x):\n","    # Shortcut connection for attention block\n","    shortcut= x\n","    x= self.norm1(x)\n","    x=self.att(x)\n","    x=self.drop_shortcut(x)\n","    x= x + shortcut\n","\n","    # Shortcut connection for feed forward block\n","    shortcut= x\n","    x= self.norm2(x)\n","    x= self.ff(x)\n","    x=self.drop_shortcut(x)\n","    x=x+shortcut\n","\n","    return x\n","\n"],"metadata":{"id":"1WIDPZo9SDxf","executionInfo":{"status":"ok","timestamp":1757060899243,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["self.trf_blocks = nn.Sequntial(*[TransformerBlock(cfg) for i in range(cfg['num_layers']) ])\n","\n","TransformerBlock(cfg) is call transformer block and get tensor by return stmt\n","* *[ ... ] → unpacks the list into separate arguments.\n","\n","* Without *, we’d be passing a single list to nn.Sequential.\n","\n","* With *, we pass each block as its own argument.\n","\n","nn.Sequential(*[A, B, C]) , nn.Sequential(A, B, C) this both are same\n","\n","and for our case A,B,C is Transformer block objects\n"],"metadata":{"id":"EUHDZoTGk4cd"}},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","  def __init__(self,cfg):\n","    super().__init__()\n","    self.token_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n","    self.pos_emb= nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n","    self.drop_emb= nn.Dropout(cfg['drop_rate'])\n","\n","    self.trf_blocks=nn.Sequential(*[TransformerBlock(cfg) for i in range(cfg['n_layers'])])\n","\n","    self.final_norm= LayerNorm(cfg['emb_dim'])\n","    self.out_head= nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n","\n","  def forward(self,in_idx):\n","    batch_size,seq_len= in_idx.shape\n","    token_emb=self.token_emb(in_idx)\n","    pos_emb= self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n","    x= token_emb + pos_emb\n","    x= self.drop_emb(x)\n","    x=self.trf_blocks(x)\n","    x=self.final_norm(x)\n","    logits=self.out_head(x)\n","    return logits\n"],"metadata":{"id":"RNFMsKiMkoT9","executionInfo":{"status":"ok","timestamp":1757060899253,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","batch = []\n","txt1 = \"Every effort moves you\"\n","txt2 = \"Every day holds a\"\n","batch.append(torch.tensor(tokenizer.encode(txt1)))\n","batch.append(torch.tensor(tokenizer.encode(txt2)))\n","batch = torch.stack(batch, dim=0)\n","print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxlBiw7ubnnc","executionInfo":{"status":"ok","timestamp":1757060905107,"user_tz":-330,"elapsed":5853,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"28b4c843-ad0f-49ec-b5ed-05fb04c8b563"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","out = model(batch)\n","print(\"Input batch:\\n\", batch)\n","print(\"\\nOutput shape:\", out.shape)\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyS4570UZond","executionInfo":{"status":"ok","timestamp":1757060906481,"user_tz":-330,"elapsed":1356,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"8d2b6494-e814-46a4-b274-6ff4b56ee12b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Input batch:\n"," tensor([[6109, 3626, 6100,  345],\n","        [6109, 1110, 6622,  257]])\n","\n","Output shape: torch.Size([2, 4, 50257])\n","tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n","         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n","         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n","         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n","\n","        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n","         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n","         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n","         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n","       grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"markdown","source":["## with torch.no_grand():</br>\n","Meaning: don’t track gradients for the operations inside this block.\n","\n","Why? Because at inference (prediction) time, we don’t need backpropagation → saves memory and speeds things up."],"metadata":{"id":"Ze6D6UpSekhZ"}},{"cell_type":"code","source":["def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    for i in range(max_new_tokens):\n","        # Crop current context if it exceeds the supported context size\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Extract last row from logits (batch, vocab_size)\n","        logits = logits[:, -1, :]\n","\n","        # Apply softmax to get probabilities\n","        probs = torch.softmax(logits, dim=-1)\n","\n","        # Greedy decode: choose highest probability token\n","        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n","\n","        # Append new token\n","        idx = torch.cat((idx, idx_next), dim=1)\n","\n","    return idx   # ✅ return after loop\n"],"metadata":{"id":"D5kyOnEibru5","executionInfo":{"status":"ok","timestamp":1757060906660,"user_tz":-330,"elapsed":169,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["start_context = \"Hello, I am\"\n","encoded = tokenizer.encode(start_context)\n","print(\"encoded:\", encoded)\n","encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n","print(\"encoded_tensor.shape:\", encoded_tensor.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NxxeNH7jxhV","executionInfo":{"status":"ok","timestamp":1757060906698,"user_tz":-330,"elapsed":29,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"a9d8b38e-d50f-4a91-8e65-9dd0c11159c5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["encoded: [15496, 11, 314, 716]\n","encoded_tensor.shape: torch.Size([1, 4])\n"]}]},{"cell_type":"markdown","source":["model.eval() </br>\n","put model into testing phase and skip layer like normlization and dropout"],"metadata":{"id":"E-5TE4moo5bo"}},{"cell_type":"code","source":["model.eval() #A\n","out = generate_text_simple(\n","model=model,\n","idx=encoded_tensor,\n","max_new_tokens=6,\n","context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","print(\"Output:\", out)\n","print(\"Output length:\", len(out[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iB3DnhkDkAQb","executionInfo":{"status":"ok","timestamp":1757060907215,"user_tz":-330,"elapsed":517,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"38860a70-ba14-4eaa-c4a2-cae46332fb14"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n","Output length: 10\n"]}]},{"cell_type":"code","source":["decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","print(decoded_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJTSZtSQkEys","executionInfo":{"status":"ok","timestamp":1757060907321,"user_tz":-330,"elapsed":38,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"f403e415-ce65-4b22-88d0-ab0ced25f341"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, I am Featureiman Byeswickattribute argue\n"]}]},{"cell_type":"code","source":["import tiktoken\n","\n","def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0) # remove batch dimension\n","    return tokenizer.decode(flat.tolist())\n","\n","start_context = \"Every effort moves you\"\n","\n","\n","\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkYWV4mwlORE","executionInfo":{"status":"ok","timestamp":1757060908704,"user_tz":-330,"elapsed":1377,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"941ec962-dec6-4773-bae2-2501af827e4d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"]}]},{"cell_type":"code","source":["inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n","                       [40,    1107, 588]])   #  \"I really like\"]\n","\n","targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n","                        [1107,  588, 11311]]) #  \" really like chocolate\"]"],"metadata":{"id":"KRt3k1NGg_DB","executionInfo":{"status":"ok","timestamp":1757060908806,"user_tz":-330,"elapsed":101,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    logits = model(inputs)\n","\n","probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n","print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkWNWxyk0oO","executionInfo":{"status":"ok","timestamp":1757060908848,"user_tz":-330,"elapsed":40,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"ad11f53e-ead3-474f-94c8-cae398484df7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 50257])\n"]}]},{"cell_type":"code","source":["token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n","print(\"Token IDs:\\n\", token_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SrRqG0ak2Bb","executionInfo":{"status":"ok","timestamp":1757060908893,"user_tz":-330,"elapsed":44,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"3d0218cb-2d9a-4038-a597-0a35505838e3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[[36397],\n","         [39619],\n","         [20610]],\n","\n","        [[ 8615],\n","         [49289],\n","         [47105]]])\n"]}]},{"cell_type":"code","source":["print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n","print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnA7FDTApS0X","executionInfo":{"status":"ok","timestamp":1757060908918,"user_tz":-330,"elapsed":16,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"520e9a53-b5df-4afa-80d3-e1d0652c775d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Targets batch 1:  effort moves you\n","Outputs batch 1:  Gathering SerbianFriday\n"]}]},{"cell_type":"code","source":["text_idx=0\n","target_probas_1= probas[text_idx,[0,1,2],targets[text_idx]]\n","print(\"Text 1:\", target_probas_1)\n","\n","text_idx=1\n","target_probas_2=probas[text_idx,[0,1,2],targets[text_idx]]\n","print(\"Text 2:\",target_probas_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"alEFQgqSrD5u","executionInfo":{"status":"ok","timestamp":1757060909153,"user_tz":-330,"elapsed":233,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"ebcbf6de-09ce-4d3d-fc5b-12d7ac86215a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05])\n","Text 2: tensor([4.2794e-05, 1.6248e-05, 1.1586e-05])\n"]}]},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-warning\">\n","\n","We want to maximize all these values, bringing them close to a probability of 1.\n","    \n","In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself.\n","\n","</div>"],"metadata":{"id":"qDQj8sLrvmrR"}},{"cell_type":"code","source":["log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n","print(log_probas)\n","# Calculate the average probability for each token\n","avg_log_probas = torch.mean(log_probas)\n","print(avg_log_probas)\n","\n","neg_avg_log_probas = avg_log_probas * -1\n","print(neg_avg_log_probas)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PAY0RBoNvoGC","executionInfo":{"status":"ok","timestamp":1757060909623,"user_tz":-330,"elapsed":322,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"74dfb115-29f7-4e9d-81b3-13c9cd41c387"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n","tensor(-10.8765)\n","tensor(10.8765)\n"]}]},{"cell_type":"markdown","source":["We can do upper cell process in a single line of code"],"metadata":{"id":"-egdEqWawFE2"}},{"cell_type":"code","source":["logits_flat = logits.flatten(0, 1)\n","targets_flat = targets.flatten()\n","\n","print(\"Flattened logits:\", logits_flat.shape)\n","print(\"Flattened targets:\", targets_flat.shape)\n","loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pH9IKHBPwOSO","executionInfo":{"status":"ok","timestamp":1757060909624,"user_tz":-330,"elapsed":170,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"043545dd-2374-42f2-f8d5-c0227bf8e80d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Flattened logits: torch.Size([6, 50257])\n","Flattened targets: torch.Size([6])\n","tensor(10.8765)\n"]}]},{"cell_type":"markdown","source":["Perpelxity of loss"],"metadata":{"id":"9_2Yq0ABwiOp"}},{"cell_type":"code","source":["perplexity = torch.exp(loss)\n","print(perplexity)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"paIluFm0whmb","executionInfo":{"status":"ok","timestamp":1757060909625,"user_tz":-330,"elapsed":107,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"4206af79-66d3-4dba-b5a6-11c4af28564c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(52918.7773)\n"]}]},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","file_path = \"/content/drive/MyDrive/the-verdict.txt\"\n","url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n","\n","if not os.path.exists(file_path):\n","    with urllib.request.urlopen(url) as response:\n","        text_data = response.read().decode('utf-8')\n","    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n","        file.write(text_data)\n","else:\n","    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","        text_data = file.read()\n","\n","print(text_data[:99])\n","total_characters = len(text_data)\n","total_tokens = len(tokenizer.encode(text_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSRgmjcl0u4X","executionInfo":{"status":"ok","timestamp":1757060909923,"user_tz":-330,"elapsed":313,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"bd0e47aa-9a6d-465c-c73b-97f5035ec2eb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset,DataLoader\n","import tiktoken\n","\n","class GPTdatasetV1(Dataset):\n","  def __init__(self,text,tokenizer,max_length,stride) :\n","     super().__init__()\n","     self.input_ids=[]\n","     self.target_ids=[]\n","     token_ids =tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n","\n","     for i in range(0,len(token_ids)-max_length,stride):\n","        input_chunks=token_ids[i:i+max_length]\n","        target_chunks=token_ids[i+1:i+max_length+1]\n","        self.input_ids.append(torch.tensor(input_chunks))\n","        self.target_ids.append(torch.tensor(target_chunks))\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self,idx):\n","    return self.input_ids[idx],self.target_ids[idx]\n","\n","def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=None):\n","  tokenizer= tiktoken.get_encoding(\"gpt2\")\n","\n","  dataset= GPTdatasetV1(txt,tokenizer,max_length,stride)\n","\n","  dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","  return dataloader"],"metadata":{"id":"rSFZfWKKpXpx","executionInfo":{"status":"ok","timestamp":1757060909924,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,   # Vocabulary size\n","    \"context_length\": 256, # Shortened context length (orig: 1024)\n","    \"emb_dim\": 768,        # Embedding dimension\n","    \"n_heads\": 12,         # Number of attention heads\n","    \"n_layers\": 12,        # Number of layers\n","    \"drop_rate\": 0.1,      # Dropout rate\n","    \"qkv_bias\": False      # Query-key-value bias\n","}\n"],"metadata":{"id":"UJMwYmcg0cIu","executionInfo":{"status":"ok","timestamp":1757060912637,"user_tz":-330,"elapsed":2711,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Train/validation ratio\n","import torch\n","train_ratio = 0.90\n","split_idx = int(train_ratio * len(text_data))\n","train_data = text_data[:split_idx]\n","val_data = text_data[split_idx:]\n","\n","\n","torch.manual_seed(123)\n","\n","train_loader = create_dataloader_v1(\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_loader = create_dataloader_v1(\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0\n",")"],"metadata":{"id":"GSvRBX9H1S0D","executionInfo":{"status":"ok","timestamp":1757060912683,"user_tz":-330,"elapsed":48,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Sanity check\n","\n","if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the training loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"increase the `training_ratio`\")\n","\n","if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for the validation loader. \"\n","          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n","          \"decrease the `training_ratio`\")"],"metadata":{"id":"KG1gud6G1692","executionInfo":{"status":"ok","timestamp":1757060912686,"user_tz":-330,"elapsed":45,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["print(\"Train loader:\")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"\\nValidation loader:\")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)\n","\n","print(len(train_loader))\n","print(len(val_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf-Iw-sD2J8r","executionInfo":{"status":"ok","timestamp":1757060916151,"user_tz":-330,"elapsed":41,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"2d4a9b68-2606-4a3d-acc7-585dda2e1e06"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Train loader:\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","torch.Size([2, 256]) torch.Size([2, 256])\n","\n","Validation loader:\n","torch.Size([2, 256]) torch.Size([2, 256])\n","9\n","1\n"]}]},{"cell_type":"code","source":["train_tokens= 0\n","for input_batch,traget_batch in train_loader:\n","  train_tokens+= input_batch.numel()\n","\n","val_tokens=0\n","for input_batch,target_batch in val_loader:\n","  val_tokens += input_batch.numel()\n","\n","print(\"Training tokens:\", train_tokens)\n","print(\"Validation tokens:\", val_tokens)\n","print(\"All tokens:\", train_tokens + val_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1o5glXuN3Guv","executionInfo":{"status":"ok","timestamp":1757060917153,"user_tz":-330,"elapsed":1010,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"0b8f4da6-3f43-426b-a101-4c10492b3c34"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Training tokens: 4608\n","Validation tokens: 512\n","All tokens: 5120\n"]}]},{"cell_type":"code","source":["class GPTModel(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n","        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n","        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.trf_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n","\n","        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeds = self.tok_emb(in_idx)\n","        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n","        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n","        x = self.drop_emb(x)\n","        x = self.trf_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval();  # Disable dropout during inference"],"metadata":{"id":"Fos8F9av4Crx","executionInfo":{"status":"ok","timestamp":1757060917207,"user_tz":-330,"elapsed":18,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":[".to(device)\n","\n","In PyTorch, .to(device) moves a tensor to a specified device.\n","\n","device is usually either:\n","\n","\"cpu\" → keep tensor on CPU\n","\n","\"cuda\" → move tensor to GPU for faster trainin"],"metadata":{"id":"B8pCLenc4sJO"}},{"cell_type":"code","source":["def calc_loss_batch(input_batch,target_batch,model,device):\n","  input_batch,target_batch= input_batch.to(device),target_batch.to(device)\n","  logits= model(input_batch)\n","  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n","  return loss\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def calc_loss_loader(data_loader,model,device,num_batch=None):\n","    total_loss= 0.\n","    if  len(data_loader) == 0:\n","      return \"Zero input\"\n","    elif num_batch is None:\n","      num_batch= len(data_loader)\n","    else:\n","      # Reduce the number of batches to match the total number of batches in the data loader\n","      # if num_batches exceeds the number of batches in the data loader\n","      num_batch= min(num_batch,len(data_loader))\n","\n","    for i,(input_batch,target_batch) in enumerate(data_loader):\n","      if i < num_batch:\n","        loss= calc_loss_batch(input_batch,target_batch,model,device)\n","        total_loss +=loss.item()\n","      else:\n","        break\n","    return total_loss/num_batch\n","\n"],"metadata":{"id":"X3vdfjml4U7L","executionInfo":{"status":"ok","timestamp":1757060917258,"user_tz":-330,"elapsed":50,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n","\n","\n","torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n","\n","with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n","    train_loss = calc_loss_loader(train_loader, model, device)\n","    val_loss = calc_loss_loader(val_loader, model, device)\n","\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5D4u5pXf5Lb","executionInfo":{"status":"ok","timestamp":1757060917340,"user_tz":-330,"elapsed":56,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"bdf319fd-379f-4c39-d780-260936c397de"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss: 10.987583372328016\n","Validation loss: 10.983159065246582\n"]}]},{"cell_type":"markdown","source":["<h1>Training of llm</h1>"],"metadata":{"id":"T40FOjrhofMx"}},{"cell_type":"code","source":["def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n","  model.eval() #Set model to evalution mode\n","  with torch.no_grad():\n","    train_loss=calc_loss_loader(train_loader,model,device,num_batch=eval_iter)\n","    val_loss=calc_loss_loader(val_loader,model,device,num_batch=eval_iter)\n","    model.train() #set model to train mode\n","    return train_loss,val_loss"],"metadata":{"id":"Bff-cxwzxH7D","executionInfo":{"status":"ok","timestamp":1757060917392,"user_tz":-330,"elapsed":56,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def generate_and_print_sample(model,toeknizer,device,start_context):\n","  model.eval()\n","  context_size= model.pos_emb.weight.shape[0]\n","  encoded = text_to_token_ids(start_context,tokenizer).to(device)\n","  with torch.no_grad():\n","    token_ids=generate_text_simple(model=model,idx= encoded,max_new_tokens=50,context_size=context_size)\n","  decoded_text= token_ids_to_text(token_ids,tokenizer)\n","  print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n","  model.train()"],"metadata":{"id":"Jdfl5IA6yqxQ","executionInfo":{"status":"ok","timestamp":1757060917426,"user_tz":-330,"elapsed":22,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n","                       eval_freq, eval_iter, start_context, tokenizer):\n","    # Initialize lists to track losses and tokens seen\n","    train_losses, val_losses, track_tokens_seen = [], [], []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main training loop\n","    for epoch in range(num_epochs):\n","        model.train()  # Set model to training mode\n","\n","        for input_batch, target_batch in train_loader:\n","            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n","            loss = calc_loss_batch(input_batch, target_batch, model, device)\n","            loss.backward() # Calculate loss gradients\n","            optimizer.step() # Update model weights using loss gradients\n","            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n","            global_step += 1\n","\n","            # Optional evaluation step\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter)\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n","\n","        # Print a sample text after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","\n","    return train_losses, val_losses, track_tokens_seen"],"metadata":{"id":"51x8lRBDomiM","executionInfo":{"status":"ok","timestamp":1757060917520,"user_tz":-330,"elapsed":83,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["### loss.backward():\n"," use for calculating loss by partial derivative of loss with respect to all weights\n","\n","\n","### optimizer.step():\n","Update weights values\n","\n","### if global_step % eval_freq == 0:\n","means print losses after some ietrarion if eval_freq= 5 means print losses after every 5 iteration"],"metadata":{"id":"ZIb5cR8C7Hnp"}},{"cell_type":"code","source":["# Note:\n","# Uncomment the following code to calculate the execution time\n","import time\n","start_time = time.time()\n","\n","torch.manual_seed(123)\n","model = GPTModel(GPT_CONFIG_124M)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","train_losses, val_losses, tokens_seen = train_model_simple(\n","    model, train_loader, val_loader, optimizer, device,\n","    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",")\n","\n","# Note:\n","# Uncomment the following code to show the execution time\n","end_time = time.time()\n","execution_time_minutes = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_INOm3s72evo","executionInfo":{"status":"ok","timestamp":1757060949199,"user_tz":-330,"elapsed":31706,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"fc29e417-4e8a-4c67-cb6b-5c8c9a096c76"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Ep 1 (Step 000000): Train loss 9.818, Val loss 9.930\n","Ep 1 (Step 000005): Train loss 8.066, Val loss 8.336\n","Every effort moves you,,,,,,,,,,,,.                                     \n","Ep 2 (Step 000010): Train loss 6.623, Val loss 7.053\n","Ep 2 (Step 000015): Train loss 6.047, Val loss 6.605\n","Every effort moves you, and,, and,,,,,,, and,.                                   \n","Ep 3 (Step 000020): Train loss 5.532, Val loss 6.509\n","Ep 3 (Step 000025): Train loss 5.399, Val loss 6.389\n","Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had the, and, and, and, and, and, and, and, and, and\n","Ep 4 (Step 000030): Train loss 4.895, Val loss 6.280\n","Ep 4 (Step 000035): Train loss 4.648, Val loss 6.305\n","Every effort moves you.  \"I the picture.                    \"I\"I the picture\"I had the the honour of the picture and I had been the picture of\n","Ep 5 (Step 000040): Train loss 4.023, Val loss 6.165\n","Every effort moves you know                                                 \n","Ep 6 (Step 000045): Train loss 3.625, Val loss 6.172\n","Ep 6 (Step 000050): Train loss 3.045, Val loss 6.143\n","Every effort moves you know the was his a little the.  \"I had the last word.           \"Oh, and I had a little.   \"I looked, and I had a little of\n","Ep 7 (Step 000055): Train loss 2.948, Val loss 6.183\n","Ep 7 (Step 000060): Train loss 2.230, Val loss 6.128\n","Every effort moves you know the picture to have been too--I felt, and Mrs.  \"I was no--and the fact, and that, and I was his pictures.  \"I looked up his pictures--and--because he was a little\n","Ep 8 (Step 000065): Train loss 1.774, Val loss 6.162\n","Ep 8 (Step 000070): Train loss 1.475, Val loss 6.229\n","Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up, and the fact, and to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n","Ep 9 (Step 000075): Train loss 1.135, Val loss 6.267\n","Ep 9 (Step 000080): Train loss 0.858, Val loss 6.297\n","Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word.    \"I looked, and that, and I remember getting off a prodigious phrase about the honour being _mine_--because he's the first\n","Ep 10 (Step 000085): Train loss 0.627, Val loss 6.382\n","Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n","Training completed in 0.61 minutes.\n"]}]}]}