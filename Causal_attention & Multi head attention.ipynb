{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhaxMrDqab+MygNiLJGOki"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")\n","d_in = inputs.shape[1] #B\n","\n","print(d_in)\n","d_out = 2 #C"],"metadata":{"id":"ukpCQsb05bNC","executionInfo":{"status":"ok","timestamp":1755777962905,"user_tz":-330,"elapsed":6136,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"85b33a08-1fe0-4a37-d697-efb38a15748c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","class SelfAttention_v2(nn.Module):\n","\n","    def __init__(self, d_in, d_out, qkv_bias=False):\n","        super().__init__()\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","        print(self.W_query)\n","    def forward(self, x):\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        attn_scores = queries @ keys.T\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","\n","        context_vec = attn_weights @ values\n","        return context_vec\n","\n","torch.manual_seed(789)\n","sa_v2 = SelfAttention_v2(d_in, d_out)\n","print(sa_v2(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AgKqCirwzYe9","executionInfo":{"status":"ok","timestamp":1755777963078,"user_tz":-330,"elapsed":180,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"b3e3a4c1-a943-4ca1-b802-1499ce6abc06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear(in_features=3, out_features=2, bias=False)\n","tensor([[-0.0739,  0.0713],\n","        [-0.0748,  0.0703],\n","        [-0.0749,  0.0702],\n","        [-0.0760,  0.0685],\n","        [-0.0763,  0.0679],\n","        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"code","source":["queries = sa_v2.W_query(inputs) #A\n","keys = sa_v2.W_key(inputs)\n","# print(keys)\n","attn_scores = queries @ keys.T\n","attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n","print(attn_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNWPC7lCy_6X","executionInfo":{"status":"ok","timestamp":1755777963090,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"093741e7-4075-4049-ccb2-a49676cb0bb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n","        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n","        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n","        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n","        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["context_length=6\n","torch.ones(context_length, context_length)"],"metadata":{"id":"FSdWp9oSJGOu","executionInfo":{"status":"ok","timestamp":1755777963108,"user_tz":-330,"elapsed":16,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"515044e6-267c-4720-aee4-b2b4812bea76","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["When negative\n","infinity values (-âˆž) are present in a row, the softmax function treats them as zero\n","probability."],"metadata":{"id":"LtK2bw2Izoj7"}},{"cell_type":"code","source":["mask= torch.triu(torch.ones(context_length,context_length),diagonal= 1) # exclude diagonal value for that diagoanl= 1\n","masked= attn_scores.masked_fill(mask.bool(),-torch.inf)\n","print(masked)"],"metadata":{"id":"uBzz-SH-JHhb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755777963301,"user_tz":-330,"elapsed":187,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"1084fb8c-2469-4c83-948e-0cb604a98772"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n","        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n","        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n","        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n","        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n","        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n","       grad_fn=<MaskedFillBackward0>)\n"]}]},{"cell_type":"code","source":["atten_weight= torch.softmax(masked/ keys.shape[-1]**0.5 ,dim=1)\n","print(atten_weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qun_ID--0jqS","executionInfo":{"status":"ok","timestamp":1755777963323,"user_tz":-330,"elapsed":24,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"93feccd7-a52e-4457-d903-39043f5ab476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n","        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n","        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n","        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n","       grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["torch.manual_seed(123)\n","dropout= torch.nn.Dropout(0.5)\n","ones= torch.ones(6,6)\n","exmple= dropout(ones)\n","print(exmple)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcpG0grd1QEA","executionInfo":{"status":"ok","timestamp":1755777963332,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"f58b9653-ed52-4c69-8673-9d8348c49f22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 2., 0., 2., 2., 0.],\n","        [0., 0., 0., 2., 0., 2.],\n","        [2., 2., 2., 2., 0., 2.],\n","        [0., 2., 2., 0., 0., 2.],\n","        [0., 2., 0., 2., 0., 2.],\n","        [0., 2., 2., 2., 2., 0.]])\n"]}]},{"cell_type":"markdown","source":["this type of dropout is implement on atten weight matrix as we put dropout=50% it dropout value averagly 50% from matrix<br>\n","In PyTorch nn.Module, register_buffer tells the model:\n","\n","\"mask\" is not a learnable parameter (not updated by backpropagation).\n","\n","But it moves with the model (goes to GPU with .cuda() or .to(device) and gets saved/loaded with state_dict)."],"metadata":{"id":"1eE1oGGw25ej"}},{"cell_type":"markdown","source":["3. self.W_key(x)\n","\n","When you call it with input x, it performs:\n","\n","keys= self.w_key(x) means keys= x*weight + b\n","So for each input token embedding (dimension = d_in), it creates a new vector (dimension = d_k) called the key."],"metadata":{"id":"dkQ37yNJHjqu"}},{"cell_type":"markdown","source":["in register_buffer (mask is variable with tensor which is upper tringular)\n"],"metadata":{"id":"f4OJjFFJfGpl"}},{"cell_type":"code","source":["class CausalAttention(nn.Module):\n","  def __init__(self, d_in,d_out,context_length,dropout,qkv_bias=False) :\n","      super().__init__()\n","      self.d_out= d_out\n","      self.w_query= nn.Linear(d_in,d_out,bias= qkv_bias)\n","      self.w_key= nn.Linear(d_in,d_out,bias= qkv_bias)\n","      self.w_value= nn.Linear(d_in,d_out,bias= qkv_bias)\n","      self.dropout= nn.Dropout(dropout)\n","      self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","  def forward(self,x):\n","    b,num_tokens,d_in= x.shape # New batch dimension b\n","    keys = self.w_key(x)\n","    query= self.w_query(x)\n","    value= self.w_value(x)\n","    print(keys.shape)\n","\n","    atten_score= query @ keys.transpose(1,2)\n","    atten_score.masked_fill(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)\n","    atten_weight= torch.softmax(atten_score/keys.shape[-1]**0.5 ,dim=-1)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n","    atten_weight=self.dropout(atten_weight)\n","    print(atten_weight.shape)\n","    print(value.shape)\n","    contex_vec= atten_weight @ value\n","    return contex_vec\n","\n"],"metadata":{"id":"PHcWyAOg22VH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"KPIPnodV6Dkd"}},{"cell_type":"code","source":["batch = torch.stack((inputs, inputs), dim=0)\n","print(batch.shape)\n","torch.manual_seed(123)\n","context_length = batch.shape[1]\n","ca = CausalAttention(d_in, d_out, context_length, 0.0)\n","context_vecs = ca(batch)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"id":"W9V-_XYK3T6_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755777963364,"user_tz":-330,"elapsed":15,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"3d3a0df6-0d2a-43f7-9bbe-fb7406d0d374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 6, 3])\n","torch.Size([2, 6, 2])\n","torch.Size([2, 6, 6])\n","torch.Size([2, 6, 2])\n","context_vecs.shape: torch.Size([2, 6, 2])\n"]}]},{"cell_type":"code","source":["ca(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rhJ9m4xMSb9","executionInfo":{"status":"ok","timestamp":1755777963366,"user_tz":-330,"elapsed":8,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"7a90faa1-80dc-4803-8ba2-79167dc5dbec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 6, 2])\n","torch.Size([2, 6, 6])\n","torch.Size([2, 6, 2])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.5337, -0.1051],\n","         [-0.5323, -0.1080],\n","         [-0.5323, -0.1079],\n","         [-0.5297, -0.1076],\n","         [-0.5311, -0.1066],\n","         [-0.5299, -0.1081]],\n","\n","        [[-0.5337, -0.1051],\n","         [-0.5323, -0.1080],\n","         [-0.5323, -0.1079],\n","         [-0.5297, -0.1076],\n","         [-0.5311, -0.1066],\n","         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print(context_vecs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4Lg7EV5Ik7F","executionInfo":{"status":"ok","timestamp":1755777963386,"user_tz":-330,"elapsed":20,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"9b374012-b611-4e2f-8d7c-47919a39cc5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.5337, -0.1051],\n","         [-0.5323, -0.1080],\n","         [-0.5323, -0.1079],\n","         [-0.5297, -0.1076],\n","         [-0.5311, -0.1066],\n","         [-0.5299, -0.1081]],\n","\n","        [[-0.5337, -0.1051],\n","         [-0.5323, -0.1080],\n","         [-0.5323, -0.1079],\n","         [-0.5297, -0.1076],\n","         [-0.5311, -0.1066],\n","         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"]}]},{"cell_type":"code","source":["class Multiheadattentionwrapper(nn.Module):\n","  def __init__(self,d_in,d_out,contex_length,dropout,num_heads,qkv_bias=False):\n","    super().__init__()\n","    self.heads=nn.ModuleList([CausalAttention(d_in,d_out,contex_length,dropout,qkv_bias) for _ in range(num_heads)])\n","\n","  def forward(self,x):\n","    return torch.cat([head(x) for head in self.heads],dim=-1)\n"],"metadata":{"id":"YvdVlxDwhSFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")\n","batch = torch.stack((inputs, inputs), dim=0)\n","print(batch.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8qigIllitzS","executionInfo":{"status":"ok","timestamp":1755778311054,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"b8c734b0-2bb1-41cb-c9e8-2dd5109e59f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 6, 3])\n"]}]},{"cell_type":"markdown","source":["It gives you all the machinery for parameters, buffers, and layers to be registered automatically.\n","\n","Provides built-in methods like .parameters(), .to(device), .train(), .eval(), etc.\n","\n","Ensures your sub-layers (like Linear, Dropout, etc.) are tracked and updated when training.\n","\n","Without inheriting nn.Module, PyTorch wouldnâ€™t know which tensors are trainable parameters.\n","\n","ðŸ”¹ 2. What is nn.ModuleList?\n","\n","nn.ModuleList is a container for holding a list of layers (modules).\n","\n","Itâ€™s just like a Python list, but with one big difference:\n","PyTorch registers everything inside it as submodules of your parent module.\n","\n","That means:\n","\n","Their parameters will appear in .parameters().\n","\n","Theyâ€™ll automatically move to GPU/CPU when you call .to(device).\n","\n","Theyâ€™ll toggle correctly with .train() and .eval().\n","\n","ðŸ‘‰ If you used a normal Python list, PyTorch wouldnâ€™t track those submodules, and their parameters wouldnâ€™t update during training."],"metadata":{"id":"uIC0R4Q0j19-"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","context_length = batch.shape[1] # This is the number of tokens = 6\n","d_in, d_out = 3, 2\n","mha = Multiheadattentionwrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n","context_vecs = mha(batch)\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GP6B7UIViunA","executionInfo":{"status":"ok","timestamp":1755778332949,"user_tz":-330,"elapsed":32,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"4806cbeb-4da9-49ef-a542-2f2ba94bee1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 6, 2])\n","torch.Size([2, 6, 6])\n","torch.Size([2, 6, 2])\n","torch.Size([2, 6, 2])\n","torch.Size([2, 6, 6])\n","torch.Size([2, 6, 2])\n","tensor([[[-0.5337, -0.1051,  0.5085,  0.3508],\n","         [-0.5323, -0.1080,  0.5084,  0.3508],\n","         [-0.5323, -0.1079,  0.5084,  0.3506],\n","         [-0.5297, -0.1076,  0.5074,  0.3471],\n","         [-0.5311, -0.1066,  0.5076,  0.3446],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]],\n","\n","        [[-0.5337, -0.1051,  0.5085,  0.3508],\n","         [-0.5323, -0.1080,  0.5084,  0.3508],\n","         [-0.5323, -0.1079,  0.5084,  0.3506],\n","         [-0.5297, -0.1076,  0.5074,  0.3471],\n","         [-0.5311, -0.1066,  0.5076,  0.3446],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n","context_vecs.shape: torch.Size([2, 6, 4])\n"]}]},{"cell_type":"markdown","source":["Now wea are going to create a Multi-head attention in single class\n"],"metadata":{"id":"DF4agSBKm6dC"}},{"cell_type":"code","source":["class MultiheadAttention(nn.module):\n","  def __init__(self,d_in,d_out,contex_length,dropout,num_heads,qkv_bias=False):\n","    super().__init__()\n","    assert(d_out % num_heads ==0), \"d_out must be divisible by num_heads\"\n","    self.d_out=d_out\n","    self.num_heads= num_heads\n","    self.head_dim= d_out // num_heads\n","\n","    self.w_query=nn.Linear(d_in,d_out,qkv_bias)\n","    self.w_key=nn.Linear(d_in,d_out,qkv_bias)\n","    self.w_query=nn.Linear(d_in,d_out,qkv_bias)\n","    self.out_proj= nn.Linear(d_out,d_out)  # Linear layer to combine head outputs\n","    self.dropout= nn.Dropout(dropout)\n","    self.register_buffer(\"mask\",torch.triu(torch.ones(contex_length,context_length),diagonal=1))\n","\n","  def forward(self,x):\n","    b,num_tokens,d_in = x.shape\n","    keys =self.w_key(x) # Shape: (b, num_tokens, d_out)\n","    query= self.w_query(x)\n","    value= self.w_value(x)\n","\n","    # We implicitly split the matrix by adding a `num_heads` dimension\n","    # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\n","    keys= keys.view(b,num_token,num_heads,head_dim)\n","    query=keys.view(b,num_token,num_heads,head_dim)\n","    value=keys.view(b,num_token,num_heads,head_dim)\n","\n","    # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","    keys= keys.transpose(1,2)\n","    query=query.transpose(1,2)\n","    value=value.transpose(1,2)\n","\n","    # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","    atten_score= query @ keys.transpose(1,2)\n","\n","    # Original mask truncated to the number of tokens and converted to boolean\n","    mask_bool = self.mask.bool()[:num_token,:num_token]\n","\n","    # Use the mask to fill attention scores\n","    atten_score.masked_fill(mask_bool,-torch.inf)\n","    atten_weight=torch.softmax(atten_score / keys.shape[-1]**0.5 ,dim=-1)\n","\n","    atten_weight= self.dropout(atten_weight)\n","\n","    # Shape: (b, num_tokens, num_heads, head_dim)\n","    contex_vector= (atten_weight @ value).transpose(1,2)\n","\n","    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","    context_vec = self.out_proj(context_vec) # optional projection\n","\n","    return context_vec"],"metadata":{"id":"pkEOLvD5nD_n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. What does .contiguous() mean?\n","\n","In PyTorch, tensors are stored in memory as 1D arrays, with some stride info telling how to interpret them as multi-dimensional.\n","When you do operations like .transpose() or .view(), sometimes the tensor in memory is no longer contiguous (not stored in a simple sequential block).\n","\n","ðŸ‘‰ .contiguous() makes a new copy of the tensor in memory so that all elements are laid out sequentially again.\n","\n","2. Why is this important before .view()?\n","\n",".view() doesnâ€™t actually rearrange values, it just reinterprets memory layout with a new shape.\n","If the tensor is non-contiguous, .view() might give wrong results or raise an error.\n","\n","<h3><b>\n","Why do we care?\n","</b><br><u>\n","Some operations (like .view()) assume contiguous memory.\n","\n","If the tensor is not contiguous, they fail or give wrong results.\n","\n",".contiguous() makes a new copy in memory, arranging numbers as they appear, so you can safely reshape.\n","</u></h3>"],"metadata":{"id":"lO3x9epVcpKw"}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-info\">\n","\n","Step 1: Reduce the projection dim to match desired output dim\n","\n","Step 2: Use a Linear layer to combine head outputs\n","\n","Step 3: Tensor shape: (b, num_tokens, d_out)\n","\n","Step 4: We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b,\n","num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","\n","Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n","\n","Step 6: Compute dot product for each head\n","\n","Step 7: Mask truncated to the number of tokens\n","\n","Step 8: Use the mask to fill attention scores\n","\n","Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n","\n","Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n","\n","Step 11: Add an optional linear projection\n","</div>"],"metadata":{"id":"KFtn_qmmcveo"}}]}