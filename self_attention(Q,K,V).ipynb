{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOi/NFwuY/veKRyd4akPUPf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"7AIZIRc9Yumg","executionInfo":{"status":"ok","timestamp":1755324363026,"user_tz":-330,"elapsed":2,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"outputs":[],"source":["import torch\n","\n","inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")"]},{"cell_type":"markdown","source":["Why we use sqrt because it help to keep varince =1"],"metadata":{"id":"o_6GZF9-ZX2L"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to compute variance before and after scaling\n","def compute_variance(dim, num_trials=1000):\n","    dot_products = []\n","    scaled_dot_products = []\n","\n","    # Generate multiple random vectors and compute dot products\n","    for _ in range(num_trials):\n","        q = np.random.randn(dim)\n","        k = np.random.randn(dim)\n","\n","        # Compute dot product\n","        dot_product = np.dot(q, k)\n","        dot_products.append(dot_product)\n","\n","        # Scale the dot product by sqrt(dim)\n","        scaled_dot_product = dot_product / np.sqrt(dim)\n","        scaled_dot_products.append(scaled_dot_product)\n","\n","    # Calculate variance of the dot products\n","    variance_before_scaling = np.var(dot_products)\n","    variance_after_scaling = np.var(scaled_dot_products)\n","\n","    return variance_before_scaling, variance_after_scaling\n","\n","# For dimension 5\n","variance_before_5, variance_after_5 = compute_variance(5)\n","print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n","print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n","\n","# For dimension 20\n","variance_before_100, variance_after_100 = compute_variance(100)\n","print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n","print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbQr5PvjZNRS","executionInfo":{"status":"ok","timestamp":1755322870950,"user_tz":-330,"elapsed":392,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"69c624b0-8a82-453b-f3d1-430cf3d67115"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Variance before scaling (dim=5): 5.569993232122386\n","Variance after scaling (dim=5): 1.113998646424477\n","Variance before scaling (dim=100): 89.35346930640613\n","Variance after scaling (dim=100): 0.8935346930640611\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","class self_attentionv1(nn.Module):\n","  def __init__(self,d_in,d_out):# input and output dimension\n","    super().__init__()\n","    self.w_query=nn.Parameter(torch.rand(d_in,d_out))\n","    self.w_key=nn.Parameter(torch.rand(d_in,d_out))\n","    self.w_value=nn.Parameter(torch.rand(d_in,d_out))\n","\n","  def forward(self,x):\n","    key = x @ self.w_key\n","    query=x @ self.w_query\n","    value= x @ self.w_value\n","\n","    attention_score= query @ key.T\n","    attention_weight= torch.softmax( attention_score / key.shape[-1]**0.5 ,dim =-1)\n","    contex_vector =attention_weight @ value\n","    return contex_vector\n"],"metadata":{"id":"1S5cynFfbJ0r","executionInfo":{"status":"ok","timestamp":1755324498075,"user_tz":-330,"elapsed":412,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","d_in = inputs.shape[1]\n","d_out = 2\n","sa_v1 = self_attentionv1(d_in, d_out)\n","print(sa_v1.forward(inputs))\n","print(sa_v1(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JF3yMXs2fDns","executionInfo":{"status":"ok","timestamp":1755324704236,"user_tz":-330,"elapsed":454,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"5ca3f89d-2a6c-4b54-b170-49c039994b6d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n","tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Additionally, a significant advantage of using nn.Linear instead of manually\n","implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n","initialization scheme, contributing to more stable and effective model training."],"metadata":{"id":"e5T7SP4dgnvU"}},{"cell_type":"markdown","source":["meaning of self.w_key(x)\n","y=(x)*(W)+b\n"],"metadata":{"id":"NqDiwDJeiw39"}},{"cell_type":"code","source":["import torch.nn as nn\n","class self_attentionv2(nn.Module):\n","  def __init__(self,d_in,d_out,qkv_bias=False):# input and output dimension\n","    super().__init__()\n","    self.w_query=nn.Linear(d_in,d_out,bias= qkv_bias)\n","    self.w_key=nn.Linear(d_in,d_out,bias= qkv_bias)\n","    self.w_value=nn.Linear(d_in,d_out,bias= qkv_bias)\n","\n","  def forward(self,x):\n","    key = self.w_key(x)\n","    query=self.w_query(x)\n","    value= self.w_value(x)\n","\n","    attention_score= query @ key.T\n","    attention_weight= torch.softmax( attention_score / key.shape[-1]**0.5 ,dim =-1)\n","    contex_vector =attention_weight @ value\n","    return contex_vector\n"],"metadata":{"id":"3mVAnylCgm_v","executionInfo":{"status":"ok","timestamp":1755325294624,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","d_in = inputs.shape[1]\n","d_out = 2\n","sa_v2 = self_attentionv2(d_in, d_out)\n","print(sa_v2.forward(inputs))\n","print(sa_v2(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIBvUMgPgzKZ","executionInfo":{"status":"ok","timestamp":1755325300280,"user_tz":-330,"elapsed":9,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"f242c5b6-38a8-40f5-c191-5e4daea5bcef"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n","tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"]}]}]}