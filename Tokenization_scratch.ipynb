{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"14UI27hAr5lLnG7fBKECRwdJvI-BguKWK","authorship_tag":"ABX9TyMmiNKds2UWDRQB5Irx99em"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"zOFsDV2GVMyd","executionInfo":{"status":"ok","timestamp":1755066987613,"user_tz":-330,"elapsed":1798,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udylXmZ32WCo","executionInfo":{"status":"ok","timestamp":1755066989066,"user_tz":-330,"elapsed":24,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"68c75417-c853-4b2a-cf72-d9002dfe6019"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello,', ' ', 'harsh.', ' ', 'how', ' ', 'are', ' ', 'you?', ' ', 'harsh']"]},"metadata":{},"execution_count":1}],"source":["import re\n","text= \"hello, harsh. how are you? harsh\"\n","\n","#this is will split string from white space\n","reslt= re.split(r'(\\s)',text)\n","reslt"]},{"cell_type":"code","source":["#this will seperate comma and dot also\n","result= re.split(r'([,.]|\\s)',text)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mD3ksZQV303d","executionInfo":{"status":"ok","timestamp":1755066989067,"user_tz":-330,"elapsed":24,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"3a290502-006f-404a-e8d1-89e0d148def5"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello',\n"," ',',\n"," '',\n"," ' ',\n"," 'harsh',\n"," '.',\n"," '',\n"," ' ',\n"," 'how',\n"," ' ',\n"," 'are',\n"," ' ',\n"," 'you?',\n"," ' ',\n"," 'harsh']"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["REMOVING WHITESPACES OR NOT\n","\n","When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces."],"metadata":{"id":"b4e0dend5yKu"}},{"cell_type":"code","source":["#exclude white space\n","result= [item for item in result if item.strip()]\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcejNrvu4a1v","executionInfo":{"status":"ok","timestamp":1755066989067,"user_tz":-330,"elapsed":23,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"58987681-d2c4-4c17-bb43-b98db6364c05"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello', ',', 'harsh', '.', 'how', 'are', 'you?', 'harsh']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#also include other characters\n","text = \"This that is sample text. ? for learning --- ?\"\n","result= re.split(r'([,.:\"?]|--|\\s)',text)\n","result= [item for item in result if item.strip()]\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVnxxx7p6u2o","executionInfo":{"status":"ok","timestamp":1755066989068,"user_tz":-330,"elapsed":23,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"b886421d-57a9-41bc-b5e4-0b3c3bbe203b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This',\n"," 'that',\n"," 'is',\n"," 'sample',\n"," 'text',\n"," '.',\n"," '?',\n"," 'for',\n"," 'learning',\n"," '--',\n"," '-',\n"," '?']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["uniq_word= sorted(set(result))\n","len(uniq_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FlLS3GhUGWdB","executionInfo":{"status":"ok","timestamp":1755066989068,"user_tz":-330,"elapsed":22,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"4d24679b-3731-445f-b02f-b210098c0093"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["vocab = {token:integer for integer,token in enumerate(uniq_word)}\n","vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYMccAp6HDBu","executionInfo":{"status":"ok","timestamp":1755066989068,"user_tz":-330,"elapsed":21,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"cc521710-a169-4747-f6e7-8878595b4bad"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'-': 0,\n"," '--': 1,\n"," '.': 2,\n"," '?': 3,\n"," 'This': 4,\n"," 'for': 5,\n"," 'is': 6,\n"," 'learning': 7,\n"," 'sample': 8,\n"," 'text': 9,\n"," 'that': 10}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class simple_tokenizerv1:\n","  def __init__(self,vocab):\n","    self.int_to_string={i:s for s,i in vocab.items()}\n","    self.string_to_int= vocab\n","  def encode(self,text):\n","    pre= re.split(r'([,.:\"?]|--|\\s)',text)\n","    pre= [item for item in pre if item.strip()]\n","    id=[self.string_to_int[s] for s in pre]\n","    return id\n","  def decode(self,ids):\n","    text= \" \".join([self.int_to_string[i] for i in ids])\n","    # Replace spaces before the specified punctuations\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text\n","\n"],"metadata":{"id":"UXohzrrUg43a","executionInfo":{"status":"ok","timestamp":1755066989069,"user_tz":-330,"elapsed":21,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Additional special token:\n","If we building our llm from scratch in that case there is might be possibility that some sentence come which has word which is not present in our vocalbuary(mapping of uniq word with number) then additional special token comes into the picture.\n"],"metadata":{"id":"JOr09hHMmZy0"}},{"cell_type":"code","source":["result\n","uniq_word= sorted(list(set(result)))\n","uniq_word.extend([\"<|endoftext|>\",\"<|unknown|>\"])\n","\n","vocab= {token : integer for integer,token in enumerate(uniq_word)}\n","vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0863-45CpRey","executionInfo":{"status":"ok","timestamp":1755066989069,"user_tz":-330,"elapsed":21,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"f729a47f-f4b5-4cdb-d118-3df142c957a0"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'-': 0,\n"," '--': 1,\n"," '.': 2,\n"," '?': 3,\n"," 'This': 4,\n"," 'for': 5,\n"," 'is': 6,\n"," 'learning': 7,\n"," 'sample': 8,\n"," 'text': 9,\n"," 'that': 10,\n"," '<|endoftext|>': 11,\n"," '<|unknown|>': 12}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["class simple_tokenizerv2:\n","    def __init__(self,vocab):\n","      self.int_to_string={i:s for s,i in vocab.items()}\n","      self.string_to_int= vocab\n","    def encode(self,text):\n","      pre= re.split(r'([,.:\"?]|--|\\s)',text)\n","      pre= [item for item in pre if item.strip()]\n","      pre=[item if item in self.string_to_int else \"<|unknown|>\" for item in pre]\n","      id= [self.string_to_int[s] for s in pre]\n","      return id\n","    def decode(self,ids):\n","      text= \" \".join([self.int_to_string[i] for i in ids])\n","      # Replace spaces before the specified punctuations\n","      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","      return text\n"],"metadata":{"id":"cNBiQSnomY1N","executionInfo":{"status":"ok","timestamp":1755066989070,"user_tz":-330,"elapsed":21,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["tokenizer = simple_tokenizerv2(vocab)\n","\n","text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","\n","text = \" <|endoftext|> \".join((text1, text2))\n","\n","print(text)\n","\n","encoded= tokenizer.encode(text)\n","tokenizer.decode(encoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"J89w1yjxtadk","executionInfo":{"status":"ok","timestamp":1755066989070,"user_tz":-330,"elapsed":21,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"1d03a7ab-39b7-41bd-aed5-4b19f09c27bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]},{"output_type":"execute_result","data":{"text/plain":["'<|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|>? <|endoftext|> <|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|>.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import tiktoken"],"metadata":{"id":"THhbMIT0N0lt","executionInfo":{"status":"ok","timestamp":1755066989070,"user_tz":-330,"elapsed":19,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer= tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"5XujDPeIN_Rc","executionInfo":{"status":"ok","timestamp":1755066989977,"user_tz":-330,"elapsed":926,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","# gpt-2 is also using <|endoftext|> to seperate two rext\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dt_gwgNhOhsx","executionInfo":{"status":"ok","timestamp":1755066989978,"user_tz":-330,"elapsed":10,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"a855a7ee-d739-41ab-ee48-b99559a46c10"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["tokenizer.decode(integers)"],"metadata":{"id":"9BmtybZSO03L","executionInfo":{"status":"ok","timestamp":1755066989978,"user_tz":-330,"elapsed":8,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"ffe81855-569c-450c-b573-d369f3243657","colab":{"base_uri":"https://localhost:8080/","height":35}},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n","  raw_text =f.read()\n","enc_text =tokenizer.encode(raw_text)\n","\n","print(len(enc_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_n_nziIyVPJn","executionInfo":{"status":"ok","timestamp":1755066991652,"user_tz":-330,"elapsed":1681,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"9635dfc2-d28b-40ee-f550-7f909c634d9d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["5146\n"]}]},{"cell_type":"markdown","source":["Executing the code above will return 5145, the total number of tokens in the training set, after applying the BPE tokenizer.\n","\n","Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results in a slightly more interesting text passage in the next steps:"],"metadata":{"id":"PT0jLioeWIID"}},{"cell_type":"code","source":["enc_sample = enc_text[50:]\n","\n","context_size = 4 #length of the input\n","#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n","#to predict the next word in the sequence.\n","#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n","\n","x = enc_sample[:context_size]\n","y = enc_sample[1:context_size+1]\n","\n","print(f\"x: {x}\")\n","print(f\"y:      {y}\")\n","\n","for i in range(1, context_size+1):\n","    context = enc_sample[:i]\n","    desired = enc_sample[i]\n","\n","    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdDIJaKVWD50","executionInfo":{"status":"ok","timestamp":1755066991653,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"6e29cf1c-c118-4c68-b014-72ce990dbdea"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["x: [290, 4920, 2241, 287]\n","y:      [4920, 2241, 287, 257]\n"," and ---->  established\n"," and established ---->  himself\n"," and established himself ---->  in\n"," and established himself in ---->  a\n"]}]},{"cell_type":"markdown","source":["### stride means total size of input lenth that we have to pass into a model eg. if stride is 1 then input is 1 token at a time and output is 1 token,,  <length is total length is context length as shown above context length is 4>\n","\n","\n","-----------------------------------------------------------\n","Dataset is the parent from torch.utils.data.\n","\n","Your class inherits its features.\n","\n","PyTorch says:\n","\n","\"Fine, you inherited me — but you must implement __len__ and __getitem__.\"\n","--------------------------------------------------------------------------"],"metadata":{"id":"iEeG1REMtKTC"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","\n","class gptv1(Dataset):\n","    def __init__(self, text, tokenizer, length, stride):\n","        self.input_ids = []\n","        self.target_ids = []  # keep consistent naming\n","\n","        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","        for i in range(0, len(token_ids) - length, stride):\n","            input_chunk = token_ids[i:i + length]\n","            target_chunk = token_ids[i + 1: i + length + 1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n"],"metadata":{"id":"EiRTDa9-WYaT","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":3886,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["droplast because sometime you don't have same batch size some time last batch not equal to other batch so drop it </br>\n","num workers means numer of thread you want to use</br>\n","gpt use max_length=256</br>\n"],"metadata":{"id":"8DRzycAXGXbx"}},{"cell_type":"code","source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                          stride=128, shuffle=True, drop_last=True,\n","                          num_workers=0):\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = gptv1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n"],"metadata":{"id":"HKEcUjd-W42M","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":19,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()"],"metadata":{"id":"7p70bv_NHIsq","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":18,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n"],"metadata":{"id":"YeajdY79IKw_"}},{"cell_type":"code","source":["import torch\n","\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9i57u_H3IN8G","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":18,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"5e3b4702-f8a7-4645-d11b-463a0747c6e5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"]}]},{"cell_type":"code","source":["import torch\n","input_ids = torch.tensor([2, 3, 5, 1])"],"metadata":{"id":"UVyLZzfnXlPQ","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":17,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# **concept of embedding**"],"metadata":{"id":"L9SW4BT5nRmt"}},{"cell_type":"markdown","source":["torch.manual_seed(123) sets the random number generator seed so that PyTorch produces the same random values every time you run the code.\n","\n","In your example, it makes the Embedding layer’s initial weights identical on each run — useful for reproducibility in experiments or debugging.\n","\n"],"metadata":{"id":"wFNnWa1lXj6r"}},{"cell_type":"code","source":["vocab_size = 6\n","output_dim = 3\n","\n","torch.manual_seed(13)\n","embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n","\n","print(embedding_layer.weight)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVh9ozzLXoGd","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":17,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"c88bc79e-21d3-4a0e-96fb-78e7ad929341"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.4372,  0.3701, -1.5918],\n","        [-0.0736, -2.5141,  0.1140],\n","        [ 0.9822,  0.0681, -2.1701],\n","        [ 0.4161,  1.0441, -0.5201],\n","        [ 0.8059,  1.0867,  0.2593],\n","        [ 1.8514, -1.7423, -0.7882]], requires_grad=True)\n"]}]},{"cell_type":"code","source":["print(embedding_layer(torch.tensor([3])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LtnXHLmXun2","executionInfo":{"status":"ok","timestamp":1755066995535,"user_tz":-330,"elapsed":16,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"d4ee78a0-b64e-4ac5-eb5b-aa1f8598d0b7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.4161,  1.0441, -0.5201]], grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"code","source":["#since our input is torch.tensor([2, 3, 5, 1]) hence it go through above lookup table(which has embedding layer) and print embedding for input ids\n","print(embedding_layer(input_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwIgEO4YX9SJ","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":15,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"c76c4fcc-0644-496a-865e-802388da4002"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.9822,  0.0681, -2.1701],\n","        [ 0.4161,  1.0441, -0.5201],\n","        [ 1.8514, -1.7423, -0.7882],\n","        [-0.0736, -2.5141,  0.1140]], grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"markdown","source":["**Positinal embedding**"],"metadata":{"id":"tCNW-dc_xBfW"}},{"cell_type":"code","source":["vocab_size=50257 # same as gpt vocab size\n","output_dim= 256\n","\n","#generate 50257*256 this size embedding\n","token_embedding_layer= torch.nn.Embedding(vocab_size,output_dim)#(row,column)\n","token_embedding_layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acfmNJMSxAmb","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":14,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"5c5f3956-d511-4eb3-dfb3-051a57a4cef2"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(50257, 256)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["max_length= 4 # context legth\n","dataloader=  create_dataloader_v1(\n","    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False\n",")\n","data_itr= iter(dataloader)\n","inputs, targets = next(data_itr)"],"metadata":{"id":"GRYOOhxUxdB4","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["print(\"Token IDs:\\n\", inputs)\n","print(\"\\nInputs shape:\\n\", inputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XV4CEdTwyhYf","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"5a242852-1f75-4424-b849-0a145cede996"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Token IDs:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","\n","Inputs shape:\n"," torch.Size([8, 4])\n"]}]},{"cell_type":"code","source":["token_embeddings = token_embedding_layer(inputs)\n","print(token_embeddings.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AhBTj-bylGz","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":10,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"2adc4750-d083-49c5-b1ec-bf95a2f7ec27"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n"]}]},{"cell_type":"markdown","source":["# **Positinal embedding layer**\n","<hr>\n","we create only one layer of position embedding and add with token embedding bacause we have only identify token postion this is first ,this is second,etc.\n","\n","here row size is equal to contex length means we are creating (4*256) dimension position embedding output size and embedding layer output size must same"],"metadata":{"id":"XRmqfwn5y7Xu"}},{"cell_type":"code","source":["contex_length= max_length\n","pos_embedding_layer= torch.nn.Embedding(contex_length,output_dim)\n","pos_embedding_layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hj3HVwVHyv6_","executionInfo":{"status":"ok","timestamp":1755067076967,"user_tz":-330,"elapsed":449,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"10d01848-2c8b-4c22-b12b-b8eba4cb75ee"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(4, 256)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["torch.arange(max_length)\n","#this is return 0 to maxlength-1 tensor which is useful for creat"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRbi5TWIKi-C","executionInfo":{"status":"ok","timestamp":1755067333307,"user_tz":-330,"elapsed":5,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"4867c957-8631-4dd6-caa9-2695e10b5434"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n","print(pos_embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woarg6Ww1pWq","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":8,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"dc2c5871-332a-43a6-96c4-3e798364cd8c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1528, -0.4409,  0.3364,  ..., -0.1452,  1.2083, -0.2717],\n","        [-0.0043,  0.2342,  0.3640,  ...,  0.1749,  0.3848,  0.5438],\n","        [-0.2680, -0.0831, -0.5203,  ...,  1.0252, -0.4875,  1.2573],\n","        [-1.9579,  1.4795,  0.2991,  ..., -0.9544, -1.2395, -0.2830]],\n","       grad_fn=<EmbeddingBackward0>)\n"]}]},{"cell_type":"code","source":["input_embeddings = token_embeddings + pos_embeddings\n","print(input_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PW3X9QH0aQP","executionInfo":{"status":"ok","timestamp":1755066995536,"user_tz":-330,"elapsed":7,"user":{"displayName":"Harsh Tejani","userId":"04678594043684207353"}},"outputId":"6ec6a48a-52c3-4f45-c348-42c15748edee"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n"]}]}]}